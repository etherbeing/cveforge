"""
This type stub file was generated by pyright.
"""

from pygments.lexer import Lexer, RegexLexer

"""
    pygments.lexers.jvm
    ~~~~~~~~~~~~~~~~~~~

    Pygments lexers for JVM languages.

    :copyright: Copyright 2006-2024 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
"""
__all__ = [
    "JavaLexer",
    "ScalaLexer",
    "GosuLexer",
    "GosuTemplateLexer",
    "GroovyLexer",
    "IokeLexer",
    "ClojureLexer",
    "ClojureScriptLexer",
    "KotlinLexer",
    "XtendLexer",
    "AspectJLexer",
    "CeylonLexer",
    "PigLexer",
    "GoloLexer",
    "JasminLexer",
    "SarlLexer",
]

class JavaLexer(RegexLexer):
    """
    For Java source code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    version_added = ...
    flags = ...
    tokens = ...

class AspectJLexer(JavaLexer):
    """
    For AspectJ source code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    version_added = ...
    aj_keywords = ...
    aj_inter_type = ...
    aj_inter_type_annotation = ...
    def get_tokens_unprocessed(
        self, text
    ):  # -> Generator[tuple[int, _TokenType, str], Any, None]:
        ...

class ScalaLexer(RegexLexer):
    """
    For Scala source code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    version_added = ...
    flags = ...
    opchar = ...
    letter = ...
    upperLetter = ...
    letterOrDigit = ...
    letterOrDigitNoDollarSign = ...
    alphaId = ...
    simpleInterpolatedVariable = ...
    idrest = ...
    idUpper = ...
    plainid = ...
    backQuotedId = ...
    anyId = ...
    notStartOfComment = ...
    endOfLineMaybeWithComment = ...
    keywords = ...
    operators = ...
    storage_modifiers = ...
    tokens = ...

class GosuLexer(RegexLexer):
    """
    For Gosu source code.
    """

    name = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    url = ...
    version_added = ...
    flags = ...
    tokens = ...

class GosuTemplateLexer(Lexer):
    """
    For Gosu templates.
    """

    name = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    url = ...
    version_added = ...
    def get_tokens_unprocessed(
        self, text
    ):  # -> Generator[tuple[int, _TokenType, str], Any, None]:
        ...

class GroovyLexer(RegexLexer):
    """
    For Groovy source code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    version_added = ...
    flags = ...
    tokens = ...
    def analyse_text(text): ...

class IokeLexer(RegexLexer):
    """
    For Ioke (a strongly typed, dynamic,
    prototype based programming language) source.
    """

    name = ...
    url = ...
    filenames = ...
    aliases = ...
    mimetypes = ...
    version_added = ...
    tokens = ...

class ClojureLexer(RegexLexer):
    """
    Lexer for Clojure source code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    version_added = ...
    special_forms = ...
    declarations = ...
    builtins = ...
    valid_name = ...
    tokens = ...

class ClojureScriptLexer(ClojureLexer):
    """
    Lexer for ClojureScript source code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    version_added = ...

class TeaLangLexer(RegexLexer):
    """
    For Tea source code. Only used within a
    TeaTemplateLexer.

    .. versionadded:: 1.5
    """

    flags = ...
    tokens = ...

class CeylonLexer(RegexLexer):
    """
    For Ceylon source code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    version_added = ...
    flags = ...
    _ws = ...
    tokens = ...

class KotlinLexer(RegexLexer):
    """
    For Kotlin source code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    version_added = ...
    flags = ...
    kt_name = ...
    kt_space_name = ...
    kt_id = ...
    modifiers = ...
    tokens = ...

class XtendLexer(RegexLexer):
    """
    For Xtend source code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    version_added = ...
    flags = ...
    tokens = ...

class PigLexer(RegexLexer):
    """
    For Pig Latin source code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    version_added = ...
    flags = ...
    tokens = ...

class GoloLexer(RegexLexer):
    """
    For Golo source code.
    """

    name = ...
    url = ...
    filenames = ...
    aliases = ...
    version_added = ...
    tokens = ...

class JasminLexer(RegexLexer):
    """
    For Jasmin assembly code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    version_added = ...
    _whitespace = ...
    _ws = ...
    _separator = ...
    _break = ...
    _name = ...
    _unqualified_name = ...
    tokens = ...
    def analyse_text(text):  # -> float | int:
        ...

class SarlLexer(RegexLexer):
    """
    For SARL source code.
    """

    name = ...
    url = ...
    aliases = ...
    filenames = ...
    mimetypes = ...
    version_added = ...
    flags = ...
    tokens = ...
